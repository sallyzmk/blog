---
title: "아파치 스파크를 이용한 데이터 처리"
date: '2022-07-29 02:00'
---
# 아파치 스파크를 이용한 데이터 처리

### [용어 설명]

mkdir : 폴더 생성

rm -rf : 폴더 삭제

cd ..    : 직전 상위 폴더로 이동

mv **파일A** ~/**파일B** : **파일A**를 통으로 **파일B**로 옮기다. 

이때 **파일B**가 생성되어있지 않은 경우, 컴퓨터가 자동으로 **파일B**를 생성, **파일A**를 옮겨준다. 

ex) mv spark-3.2.0-bin-hadoop3.2 ~/spark3

cp -r **파일명1**/**파일명2** : **파일명1**을 복사해서 **파일명2**로 저장한다.

ex) cp -r spark3/ spark-node

cp **파일명1 파일명2 : 파일명1**을 **파일명2**

ex) cp start-master.sh start-head.sh

### 가상환경의 배경이 어디 기반인지 궁금하다면

```
# 사용하고 있는 환경을 알려줌(어느 위치에서 sourece venv/bin/activate 했는지)
which python3

# 가상환경 해제
deactivate
```

---

### 14.1 아파치 스파크의 설치와 설정 271p

```
# 참고 : 경로 /home/human/hadoop

# human 파일에 hadoop 파일 생성
mkdir hadoop

# 설치
sudo apt-get install openjdk-8-jdk

# spark-3.2.0-bin-hadoop3.2.tgz파일 다운로드
sudo wget https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz

# spark-3.2.0-bin-hadoop3.2.tgz파일 압축 해제
sudo tar -xvzf spark-3.2.0-bin-hadoop3.2.tgz

# spark-3.2.0-bin-hadoop3.2 파일을(압축 완료 상태) spark3폴더로 통째로 옮긴다.
# 참고 : 경로 /home/human/spark3
mv spark-3.2.0-bin-hadoop3.2 ~/spark3

# spark3를 복사해서 spark-node로 저장한다.
# head와 node를 구분하기 위해 이 작업을 한다.
# 원래는 컴퓨터 2대로 진행해야하나 어렵기 때문에, 두 개 똑같은 파일을 각각 head와 node로 지정해주고 진행하는 것이다.
cp -r spark3/ spark-node

cd spark3
cd sbin
ls
cp start-master.sh start-head.sh
cd ~/spark-node/sbin/
cp start-slave.sh start-node.sh

# /home/human/spark-node/sbin
cd ..
cd ..
ls
code .
```

![Untitled](images/Data_processing_using_Apache_Spark/Untitled.png)

![Untitled](images/Data_processing_using_Apache_Spark/Untitled%201.png)

[http://localhost:8080/](http://localhost:8080/)

![Untitled](images/Data_processing_using_Apache_Spark/Untitled%202.png)

---

## 14.2 PySpark의 설치와 설정 275p

### 환경설정

/usr/lib/jvm/java-8-openjdk-amd64

---

## 14.3 PySpark를 이용한 데이터 처리 277p~278p

human 파일에서

![Untitled](images/Data_processing_using_Apache_Spark/Untitled%203.png)

![Untitled](images/Data_processing_using_Apache_Spark/Untitled%204.png)

터미널 새로 열어서 `vi ~/.bashrc`

```
# 반드시 가상환경에서 설치

pip3 install findspark

pip3 install pyspark

pip3 install jupyter
```

```python
# step1.py
import pyspark

print(pyspark.__version__)
```

python3 step1.py

jupyter notebook

![Untitled](images/Data_processing_using_Apache_Spark/Untitled%205.png)

![Untitled](images/Data_processing_using_Apache_Spark/Untitled%206.png)

![Untitled](images/Data_processing_using_Apache_Spark/Untitled%207.png)

![Untitled](images/Data_processing_using_Apache_Spark/Untitled%208.png)

## 14.3.1 스파크를 이용한 데이터 공학 279p

### 가상환경에서 /home/human/airflow/dags에 있는 data.csv를 실습파일 chaper14에 복붙한다.

![Untitled](images/Data_processing_using_Apache_Spark/Untitled%209.png)

### pyspark 버전을 맞춰준다 3.3.0 → 3.2.0

![Untitled](images/Data_processing_using_Apache_Spark/Untitled%2010.png)

![Untitled](images/Data_processing_using_Apache_Spark/Untitled%2011.png)

### pandas 설치

pip3 install pandas

## jupyter notebook 실행

jupyter notebook

새 파일 만들어서 쓰기

![Untitled](images/Data_processing_using_Apache_Spark/Untitled%2012.png)

![Untitled](images/Data_processing_using_Apache_Spark/Untitled%2013.png)

### 해당빈칸에 링크를 쓰기 위해 스파크 클러스터 웹 UI를 실행한다. (교재274p,275p)

```
# 경로 /home/human/spark3/sbin에서 진행
./start-head.sh

# 경로 /home/human/spark-node/sbin에서 진행
./start-node.sh spark://pop-os.localdomain:7077 -p 991
```

![Untitled](images/Data_processing_using_Apache_Spark/Untitled%2014.png)

![Untitled](images/Data_processing_using_Apache_Spark/Untitled%2015.png)

![Untitled](images/Data_processing_using_Apache_Spark/Untitled%2016.png)

### 망함! 해결책

### 기존 코드 주석처리하고 다른 방법으로 데이터 불러옴

![Untitled](images/Data_processing_using_Apache_Spark/Untitled%2017.png)

```
import findspark
findspark.init()

import pyspark
from pyspark.sql import SparkSession

# spark = SparkSession.builder.master('spark://DESKTOP-I3TVGDG.localdomain:7077').appName('Pi-Estimation').getOrCreate()
spark = SparkSession.builder.appName('DataFrame-Kafka').getOrCreate()

df = spark.read.csv('data.csv')
df.show(5)
```

---

## pyspark tutorial 폴더 참조하여 심화 공부